# Graphiti MCP Server Environment Configuration

# FalkorDB Configs
FALKORDB_URI="redis://graphdb:6379"
FALKORDB_PASSWORD=""  # If password protected

# API Keys (1Password references)
OPENAI_API_KEY="op://Developer/graphiti/openai-api-key"
# ANTHROPIC_API_KEY="op://Developer/graphiti/anthropic-api-key"
# GOOGLE_API_KEY="op://Developer/graphiti/google-api-key"
# GROQ_API_KEY="op://Developer/graphiti/groq-api-key"
# AZURE_OPENAI_API_KEY="op://Developer/graphiti/azure-openai-api-key"

# Ollama Configuration (for local LLM/embeddings)
OLLAMA_HOST="http://ollama:11434"
OPENAI_API_BASE="http://ollama:11434/v1"

# OpenAI API Configuration
# Required for LLM operations
MODEL_NAME=llama3.2

# Ollama-compatible endpoint for embeddings
OPENAI_API_URL="http://ollama:11434/v1"

# Optional: Group ID for namespacing graph data
# GROUP_ID=my_project

# Concurrency Control
# Controls how many episodes can be processed simultaneously
# Default: 10 (suitable for OpenAI Tier 3, mid-tier Anthropic)
# Adjust based on your LLM provider's rate limits:
#   - OpenAI Tier 1 (free): 1-2
#   - OpenAI Tier 2: 5-8
#   - OpenAI Tier 3: 10-15
#   - OpenAI Tier 4: 20-50
#   - Anthropic default: 5-8
#   - Anthropic high tier: 15-30
#   - Ollama (local): 1-5
# See README.md "Concurrency and LLM Provider 429 Rate Limit Errors" for details
SEMAPHORE_LIMIT=5

# Optional: Path configuration for Docker
# PATH=/root/.local/bin:${PATH}

# Optional: Memory settings for Neo4j (used in Docker Compose)
# NEO4J_server_memory_heap_initial__size=512m
# NEO4J_server_memory_heap_max__size=1G
# NEO4J_server_memory_pagecache_size=512m

# Azure OpenAI configuration
# Optional: Only needed for Azure OpenAI endpoints
# AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint_here
# AZURE_OPENAI_API_VERSION=2025-01-01-preview
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-gpt-4o-mini-deployment
# AZURE_OPENAI_EMBEDDING_API_VERSION=2023-05-15
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=text-embedding-3-large-deployment
# AZURE_OPENAI_USE_MANAGED_IDENTITY=false
