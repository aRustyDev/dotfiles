include:
  - path: ollama-${OLLAMA_PROCESSOR_KIND:-cpu}.yaml

services:
  pull-models:
    image: ollama/ollama:latest
    profiles: ["core", "ollama"]
    stdin_open: true # docker run -i
    tty: true # docker run -t
    networks: ["global"]
    container_name: pull-models
    volumes:
      - ${XDG_DATA_HOME:?err_xdg_data_home}/mcp/ollama:/root/.ollama
    entrypoint: /bin/sh
    environment:
      - OLLAMA_HOST=ollama:11434
      - MODELS=${OLLAMA_MODELS:-llama3.2,nomic-embed-text}
    command:
      - "-c"
      - "sleep 3"
      - "ollama pull llama3.2"
      - "ollama pull nomic-embed-text"
    depends_on:
      ollama:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.1"
          memory: 128M
